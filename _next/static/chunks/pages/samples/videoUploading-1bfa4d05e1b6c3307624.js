_N_E=(window.webpackJsonp_N_E=window.webpackJsonp_N_E||[]).push([[24],{"+D0c":function(e,n){e.exports="/webgpu-samples/_next/static/fee692aa63e7fca506f31d6bd472c045.webm"},Ejpj:function(e,n,t){"use strict";t.r(n);var r=t("o0o1"),a=t.n(r),i=t("HaE+"),o=t("SoUo");function s(){return(s=Object(i.a)(a.a.mark((function e(n){var r,i,o,s,c,d,p,m,f,g,l,v;return a.a.wrap((function(e){for(;;)switch(e.prev=e.next){case 0:return(r=document.createElement("video")).loop=!0,r.autoplay=!0,r.muted=!0,r.src=t("+D0c"),e.next=7,r.play();case 7:return e.next=9,navigator.gpu.requestAdapter();case 9:return i=e.sent,e.next=12,i.requestDevice();case 12:return o=e.sent,s=n.getContext("gpupresent"),c="bgra8unorm",d=new Float32Array([1,1,0,1,0,1,-1,0,1,1,-1,-1,0,0,1,1,1,0,1,0,-1,-1,0,0,1,-1,1,0,0,0]),p=o.createBuffer({size:d.byteLength,usage:GPUBufferUsage.VERTEX,mappedAtCreation:!0}),new Float32Array(p.getMappedRange()).set(d),p.unmap(),m=s.configureSwapChain({device:o,format:c}),f=o.createRenderPipeline({vertex:{module:o.createShaderModule({code:u.vertex}),entryPoint:"main",buffers:[{arrayStride:20,attributes:[{shaderLocation:0,offset:0,format:"float32x3"},{shaderLocation:1,offset:12,format:"float32x2"}]}]},fragment:{module:o.createShaderModule({code:u.fragment}),entryPoint:"main",targets:[{format:c}]},primitive:{topology:"triangle-list"}}),g=o.createSampler({magFilter:"linear",minFilter:"linear"}),l=o.createTexture({size:{width:r.videoWidth,height:r.videoHeight},format:"rgba8unorm",usage:GPUTextureUsage.COPY_DST|GPUTextureUsage.SAMPLED}),v=o.createBindGroup({layout:f.getBindGroupLayout(0),entries:[{binding:0,resource:g},{binding:1,resource:l.createView()}]}),e.abrupt("return",(function(){createImageBitmap(r).then((function(e){o.queue.copyImageBitmapToTexture({imageBitmap:e,origin:{x:0,y:0}},{texture:l},{width:r.videoWidth,height:r.videoHeight});var n=o.createCommandEncoder(),t={colorAttachments:[{view:m.getCurrentTexture().createView(),loadValue:{r:0,g:0,b:0,a:1},storeOp:"store"}]},a=n.beginRenderPass(t);a.setPipeline(f),a.setVertexBuffer(0,p),a.setBindGroup(0,v),a.draw(6,1,0,0),a.endPass(),o.queue.submit([n.finish()])}))}));case 25:case"end":return e.stop()}}),e)})))).apply(this,arguments)}var u={vertex:"\nstruct VertexInput {\n  [[location(0)]] position : vec3<f32>;\n  [[location(1)]] uv : vec2<f32>;\n};\n\nstruct VertexOutput {\n  [[builtin(position)]] Position : vec4<f32>;\n  [[location(0)]] fragUV : vec2<f32>;\n};\n\n[[stage(vertex)]]\nfn main(input : VertexInput) -> VertexOutput {\n  return VertexOutput(vec4<f32>(input.position, 1.0), input.uv);\n}\n",fragment:"\n[[binding(0), group(0)]] var mySampler: sampler;\n[[binding(1), group(0)]] var myTexture: texture_2d<f32>;\n\n[[stage(fragment)]]\nfn main([[location(0)]] fragUV : vec2<f32>) -> [[location(0)]] vec4<f32> {\n  return textureSample(myTexture, mySampler, fragUV);\n}\n"};n.default=Object(o.c)({name:"Video Uploading",description:"This example shows how to upload video frame to WebGPU.",slug:"videoUploading",init:function(e){return s.apply(this,arguments)},source:"import { makeBasicExample } from '../../components/basicExample';\n\nasync function init(canvas: HTMLCanvasElement) {\n  // Set video element\n  const video = document.createElement('video');\n  video.loop = true;\n  video.autoplay = true;\n  video.muted = true;\n  video.src = require('../../../assets/video/pano.webm');\n  await video.play();\n\n  const adapter = await navigator.gpu.requestAdapter();\n  const device = await adapter.requestDevice();\n  const context = canvas.getContext('gpupresent');\n\n  const swapChainFormat = 'bgra8unorm';\n\n  // prettier-ignore\n  const rectVerts = new Float32Array([\n    1.0,  1.0, 0.0, 1.0, 0.0,\n    1.0, -1.0, 0.0, 1.0, 1.0,\n    -1.0, -1.0, 0.0, 0.0, 1.0,\n    1.0,  1.0, 0.0, 1.0, 0.0,\n    -1.0, -1.0, 0.0, 0.0, 1.0,\n    -1.0,  1.0, 0.0, 0.0, 0.0,\n  ]);\n\n  const verticesBuffer = device.createBuffer({\n    size: rectVerts.byteLength,\n    usage: GPUBufferUsage.VERTEX,\n    mappedAtCreation: true,\n  });\n  new Float32Array(verticesBuffer.getMappedRange()).set(rectVerts);\n  verticesBuffer.unmap();\n\n  const swapChain = context.configureSwapChain({\n    device,\n    format: swapChainFormat,\n  });\n\n  const pipeline = device.createRenderPipeline({\n    vertex: {\n      module: device.createShaderModule({\n        code: wgslShaders.vertex,\n      }),\n      entryPoint: 'main',\n      buffers: [\n        {\n          arrayStride: 20,\n          attributes: [\n            {\n              // position\n              shaderLocation: 0,\n              offset: 0,\n              format: 'float32x3',\n            },\n            {\n              // uv\n              shaderLocation: 1,\n              offset: 12,\n              format: 'float32x2',\n            },\n          ],\n        },\n      ],\n    },\n    fragment: {\n      module: device.createShaderModule({\n        code: wgslShaders.fragment,\n      }),\n      entryPoint: 'main',\n      targets: [\n        {\n          format: swapChainFormat,\n        },\n      ],\n    },\n    primitive: {\n      topology: 'triangle-list',\n    },\n  });\n\n  const sampler = device.createSampler({\n    magFilter: 'linear',\n    minFilter: 'linear',\n  });\n\n  const videoTexture = device.createTexture({\n    size: {\n      width: video.videoWidth,\n      height: video.videoHeight,\n    },\n    format: 'rgba8unorm',\n    usage: GPUTextureUsage.COPY_DST | GPUTextureUsage.SAMPLED,\n  });\n\n  const uniformBindGroup = device.createBindGroup({\n    layout: pipeline.getBindGroupLayout(0),\n    entries: [\n      {\n        binding: 0,\n        resource: sampler,\n      },\n      {\n        binding: 1,\n        resource: videoTexture.createView(),\n      },\n    ],\n  });\n\n  return function frame() {\n    createImageBitmap(video).then((videoFrame) => {\n      device.queue.copyImageBitmapToTexture(\n        { imageBitmap: videoFrame, origin: { x: 0, y: 0 } },\n        { texture: videoTexture },\n        {\n          width: video.videoWidth,\n          height: video.videoHeight,\n        }\n      );\n\n      const commandEncoder = device.createCommandEncoder();\n      const textureView = swapChain.getCurrentTexture().createView();\n\n      const renderPassDescriptor: GPURenderPassDescriptor = {\n        colorAttachments: [\n          {\n            view: textureView,\n            loadValue: { r: 0.0, g: 0.0, b: 0.0, a: 1.0 },\n            storeOp: 'store',\n          },\n        ],\n      };\n\n      const passEncoder = commandEncoder.beginRenderPass(renderPassDescriptor);\n      passEncoder.setPipeline(pipeline);\n      passEncoder.setVertexBuffer(0, verticesBuffer);\n      passEncoder.setBindGroup(0, uniformBindGroup);\n      passEncoder.draw(6, 1, 0, 0);\n      passEncoder.endPass();\n      device.queue.submit([commandEncoder.finish()]);\n    });\n  };\n}\n\nconst wgslShaders = {\n  vertex: `\nstruct VertexInput {\n  [[location(0)]] position : vec3<f32>;\n  [[location(1)]] uv : vec2<f32>;\n};\n\nstruct VertexOutput {\n  [[builtin(position)]] Position : vec4<f32>;\n  [[location(0)]] fragUV : vec2<f32>;\n};\n\n[[stage(vertex)]]\nfn main(input : VertexInput) -> VertexOutput {\n  return VertexOutput(vec4<f32>(input.position, 1.0), input.uv);\n}\n`,\n\n  fragment: `\n[[binding(0), group(0)]] var mySampler: sampler;\n[[binding(1), group(0)]] var myTexture: texture_2d<f32>;\n\n[[stage(fragment)]]\nfn main([[location(0)]] fragUV : vec2<f32>) -> [[location(0)]] vec4<f32> {\n  return textureSample(myTexture, mySampler, fragUV);\n}\n`,\n};\n\nexport default makeBasicExample({\n  name: 'Video Uploading',\n  description: 'This example shows how to upload video frame to WebGPU.',\n  slug: 'videoUploading',\n  init,\n  source: __SOURCE__,\n});\n"})},"tRp/":function(e,n,t){(window.__NEXT_P=window.__NEXT_P||[]).push(["/samples/videoUploading",function(){return t("Ejpj")}])}},[["tRp/",0,1,4,2,3]]]);